# Multi-Model Configuration Example
# This shows how to configure multiple models with kvcached for elastic memory sharing

# === kvcached Configuration (REQUIRED for multi-model support) ===
# Enables elastic GPU memory allocation between models
ENABLE_KVCACHED=true
KVCACHED_AUTOPATCH=1
KVCACHED_IPC_NAME=VLLM

# === Multi-Model Configuration (Option 1: Indexed Format) ===
# Define models using MODEL_1_NAME, MODEL_2_NAME, etc.

# Model 1: Small instruction-tuned model
MODEL_1_NAME=HuggingFaceTB/SmolLM2-135M-Instruct
MODEL_1_SERVED_NAME=smollm-135m-instruct
MODEL_1_MAX_MODEL_LEN=2048
MODEL_1_TRUST_REMOTE_CODE=false

# Model 2: Larger general-purpose model
MODEL_2_NAME=HuggingFaceTB/SmolLM2-1.7B-Instruct
MODEL_2_SERVED_NAME=smollm-1.7b-instruct
MODEL_2_MAX_MODEL_LEN=2048
MODEL_2_TRUST_REMOTE_CODE=false

# Model 3: Code-specific model (optional)
MODEL_3_NAME=HuggingFaceTB/SmolLM2-360M-Instruct
MODEL_3_SERVED_NAME=smollm-360m-instruct
MODEL_3_MAX_MODEL_LEN=1024
MODEL_3_TRUST_REMOTE_CODE=false

# === Multi-Model Configuration (Option 2: Comma-separated format) ===
# Alternative: Use comma-separated list (simpler but less per-model control)
# MODEL_NAMES=HuggingFaceTB/SmolLM2-135M-Instruct,HuggingFaceTB/SmolLM2-1.7B-Instruct,HuggingFaceTB/SmolLM2-360M-Instruct

# === Global Settings (applied to all models unless overridden) ===

# === System Performance ===
GPU_MEMORY_UTILIZATION=0.8  # Lower for multi-model to leave room for kvcached
TENSOR_PARALLEL_SIZE=1
PIPELINE_PARALLEL_SIZE=1

# === Memory and Caching ===
BLOCK_SIZE=16
SWAP_SPACE=4
ENABLE_PREFIX_CACHING=false
USE_V2_BLOCK_MANAGER=true

# === Quantization (Global) ===
QUANTIZATION=
DTYPE=auto
KV_CACHE_DTYPE=auto

# === Serverless Settings ===
MAX_CONCURRENCY=300
DISABLE_LOG_STATS=false
DISABLE_LOG_REQUESTS=false

# === Streaming Configuration ===
DEFAULT_BATCH_SIZE=50
DEFAULT_MIN_BATCH_SIZE=1
DEFAULT_BATCH_SIZE_GROWTH_FACTOR=3

# === OpenAI Compatibility ===
RAW_OPENAI_OUTPUT=1
OPENAI_RESPONSE_ROLE=assistant

# === Development ===
HF_TOKEN=your_huggingface_token_here

# === Multi-Model Usage Notes ===
# 
# Memory Considerations:
# - kvcached enables elastic memory sharing between models
# - Models are loaded lazily on first request
# - Set GPU_MEMORY_UTILIZATION lower (0.6-0.8) to allow memory flexibility
# - Monitor memory usage with kvcached CLI tools
#
# API Usage:
# - OpenAI API: Use "model" field with served_name (e.g., "smollm-135m-instruct")
# - Native API: Use "model" field with full model name or served_name
# - /v1/models endpoint lists all configured models
#
# Performance:
# - First request to each model will have higher latency (loading time)
# - Subsequent requests to loaded models are fast
# - kvcached optimizes memory allocation dynamically
#
# Example requests:
# OpenAI format: {"model": "smollm-135m-instruct", "messages": [...]}
# Native format: {"model": "HuggingFaceTB/SmolLM2-135M-Instruct", "prompt": "..."}