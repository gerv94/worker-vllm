# Single Model Configuration Example
# This shows the existing single-model setup with kvcached support

# === kvcached Configuration (for elastic GPU memory sharing) ===
ENABLE_KVCACHED=true
KVCACHED_AUTOPATCH=1
KVCACHED_IPC_NAME=VLLM

# === Model Configuration ===
MODEL_NAME=HuggingFaceTB/SmolLM2-135M-Instruct
MODEL_REVISION=main

# Optional: Custom served model name for OpenAI API
OPENAI_SERVED_MODEL_NAME_OVERRIDE=smollm-135m

# === Tokenizer Settings ===
TOKENIZER_NAME=
TOKENIZER_REVISION=main
TRUST_REMOTE_CODE=false
CUSTOM_CHAT_TEMPLATE=

# === System Performance ===
GPU_MEMORY_UTILIZATION=0.95
MAX_MODEL_LEN=2048
TENSOR_PARALLEL_SIZE=1
PIPELINE_PARALLEL_SIZE=1

# === Memory and Caching ===
BLOCK_SIZE=16
SWAP_SPACE=4
ENABLE_PREFIX_CACHING=false
USE_V2_BLOCK_MANAGER=true

# === Quantization ===
QUANTIZATION=
DTYPE=auto
KV_CACHE_DTYPE=auto

# === Serverless Settings ===
MAX_CONCURRENCY=300
DISABLE_LOG_STATS=false
DISABLE_LOG_REQUESTS=false

# === Streaming Configuration ===
DEFAULT_BATCH_SIZE=50
DEFAULT_MIN_BATCH_SIZE=1
DEFAULT_BATCH_SIZE_GROWTH_FACTOR=3

# === OpenAI Compatibility ===
RAW_OPENAI_OUTPUT=1
OPENAI_RESPONSE_ROLE=assistant

# === Development ===
HF_TOKEN=your_huggingface_token_here